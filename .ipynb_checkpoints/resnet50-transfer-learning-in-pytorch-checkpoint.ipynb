{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":25383,"databundleVersionId":2684322,"sourceType":"competition"},{"sourceId":3008839,"sourceType":"datasetVersion","datasetId":1843071}],"dockerImageVersionId":30153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pawpularity Transfer Learning Approach in Pytorch\n\nThis notebook implements a resnet50 architecture with pre-trained weights, replacing the final layer and re-training the last convlutional layer set to predict a pawpularity score bounded between 0 and 100.  Everything is implemented from scratch in pytorch.  \n\n\n- A custom pytorch dataset class is implemented to attach scores to each image file, as well as the annotations.  Currently only the images are being used to train the model.  \n- The model is a resnet50 architectur where the final fully connected layer is replaced with two fully connected layers and output 1 value.\n- The model starts with pretrained weights for all of the convolutional layers, and the final set of layers in the model (layer4) is unfrozen to allow it to learn a feature representation more specifi to this task.  Training refreezes layer4 after 4 epochs and continues to only train the fully connected layers after that to reduce overfitting.  \n- The final activation is sigmoid to bound the output between 1 and 0, and output is multiplied by 100 in the training loop to give it a bounded output between 0 and 100 which matches the range of pawpularity scores.  \n- The model is optimising for mean squared error(MSE), using Adam with weight decay to reduce overfitting.\n- The final competition evaluation metric is the square root of MSE or \n$ \\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $\n","metadata":{}},{"cell_type":"markdown","source":"### Load Dependencies","metadata":{"gradient":{"editing":false,"id":"57209741","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport time\nimport os\nfrom skimage import io, transform\nimport PIL","metadata":{"gradient":{"editing":false,"execution_count":3,"id":"e9ddbf1a","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:11.735293Z","iopub.execute_input":"2022-01-08T16:38:11.735492Z","iopub.status.idle":"2022-01-08T16:38:12.88137Z","shell.execute_reply.started":"2022-01-08T16:38:11.735467Z","shell.execute_reply":"2022-01-08T16:38:12.880643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms, models\n\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:12.883332Z","iopub.execute_input":"2022-01-08T16:38:12.88358Z","iopub.status.idle":"2022-01-08T16:38:14.572217Z","shell.execute_reply.started":"2022-01-08T16:38:12.883545Z","shell.execute_reply":"2022-01-08T16:38:14.571507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\ndata_dir = '../input/petfinder-pawpularity-score/'\nmodel_dir = '../input/resnet50-pretrained-model/'\nworking_dir = './'\nglobal_batch_size = 64\nworkers = 2\nnp.random.seed(10)\nprint(os.listdir(data_dir))\nprint(os.listdir(f'{data_dir}train')[0:4])","metadata":{"gradient":{"editing":false,"execution_count":2,"id":"e1d46881","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:14.57376Z","iopub.execute_input":"2022-01-08T16:38:14.573997Z","iopub.status.idle":"2022-01-08T16:38:14.755706Z","shell.execute_reply.started":"2022-01-08T16:38:14.573964Z","shell.execute_reply":"2022-01-08T16:38:14.755001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and Explore data","metadata":{"gradient":{"editing":false,"id":"16e2ce88","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"}}},{"cell_type":"markdown","source":"**Look at the annotations**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(f'{data_dir}train.csv')","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"f1f9f4df","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:14.757821Z","iopub.execute_input":"2022-01-08T16:38:14.758316Z","iopub.status.idle":"2022-01-08T16:38:14.793107Z","shell.execute_reply.started":"2022-01-08T16:38:14.758278Z","shell.execute_reply":"2022-01-08T16:38:14.792394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"gradient":{"editing":false,"execution_count":4,"id":"342a38d4","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:14.795501Z","iopub.execute_input":"2022-01-08T16:38:14.795772Z","iopub.status.idle":"2022-01-08T16:38:14.8153Z","shell.execute_reply.started":"2022-01-08T16:38:14.795737Z","shell.execute_reply":"2022-01-08T16:38:14.814624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:14.816717Z","iopub.execute_input":"2022-01-08T16:38:14.817137Z","iopub.status.idle":"2022-01-08T16:38:14.837796Z","shell.execute_reply.started":"2022-01-08T16:38:14.817098Z","shell.execute_reply":"2022-01-08T16:38:14.837083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Annotations\nnp.array(train_df.iloc[2, 1:13])","metadata":{"gradient":{"editing":false,"execution_count":61,"id":"da269897","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:14.839091Z","iopub.execute_input":"2022-01-08T16:38:14.839489Z","iopub.status.idle":"2022-01-08T16:38:14.845366Z","shell.execute_reply.started":"2022-01-08T16:38:14.839454Z","shell.execute_reply":"2022-01-08T16:38:14.844568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores\ntrain_df.iloc[2, 13]","metadata":{"gradient":{"editing":false,"execution_count":27,"id":"791a08a2","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:14.846733Z","iopub.execute_input":"2022-01-08T16:38:14.847269Z","iopub.status.idle":"2022-01-08T16:38:14.855138Z","shell.execute_reply.started":"2022-01-08T16:38:14.847232Z","shell.execute_reply":"2022-01-08T16:38:14.854486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n, bins, patches = plt.hist(train_df.iloc[:, 13], 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Pawpularity Histogram')\nplt.xlim(0, 100)\n# plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:14.856441Z","iopub.execute_input":"2022-01-08T16:38:14.856651Z","iopub.status.idle":"2022-01-08T16:38:15.21635Z","shell.execute_reply.started":"2022-01-08T16:38:14.856628Z","shell.execute_reply":"2022-01-08T16:38:15.215721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Custom dataset class to attach annotations and scores to the images**\n\nThis is a critical step to attach the classes and annotations to the image files and allow this to be put into a pytorch dataloader.  ","metadata":{}},{"cell_type":"code","source":"class PawpularityDataset(Dataset):\n    \"\"\"Dataset connecting animal images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n        \n        # Columns 1 to 12 contain the annotations\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n        # Column 13 has the scores\n        score = np.array(self.annotations_csv.iloc[idx, 13])\n        score = torch.tensor(score.astype('float')).view(1).to(torch.float32)\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations, score]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:15.218786Z","iopub.execute_input":"2022-01-08T16:38:15.219186Z","iopub.status.idle":"2022-01-08T16:38:15.229049Z","shell.execute_reply.started":"2022-01-08T16:38:15.219148Z","shell.execute_reply":"2022-01-08T16:38:15.228332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define global image transforms**","metadata":{}},{"cell_type":"code","source":"# Test out the transforms on an image (images need to be made the same size for the dataset to work)\n# Apply some image augmentation on the training set (rotation, flip)\n# Normalize using imagenet RGB mean and std\n\nimg_transforms = transforms.Compose([transforms.Resize(255),\n                                     transforms.CenterCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.RandomRotation(20),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225])])\n\nimg_transforms_valid = transforms.Compose([transforms.Resize(255),\n                                           transforms.CenterCrop(224),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                std=[0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:15.230205Z","iopub.execute_input":"2022-01-08T16:38:15.231068Z","iopub.status.idle":"2022-01-08T16:38:15.240845Z","shell.execute_reply.started":"2022-01-08T16:38:15.231005Z","shell.execute_reply":"2022-01-08T16:38:15.240004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load and check out the datasets**","metadata":{}},{"cell_type":"code","source":"## Load and set up the final training and validation dataset (use different transforms)\n\ntrain_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms)\nvalid_data = PawpularityDataset(f'{data_dir}train.csv', f'{data_dir}train', transform=img_transforms_valid)\n\nnp.random.seed(13)\n\n# obtain random indices that will be used for traingin/validation split\nvalid_size = 0.1\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=global_batch_size,\n                                           sampler=train_sampler, num_workers=workers,\n                                           pin_memory=True) \n# sample the validation dataset from a separate dataset the doesn't include the image aug transformations.\nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=global_batch_size,\n                                           sampler=valid_sampler, num_workers=workers,\n                                           pin_memory=True) \n\nprint(len(train_loader)*global_batch_size)\nprint(len(valid_loader)*global_batch_size)","metadata":{"gradient":{"execution_count":474,"id":"d8c207dd","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:15.242131Z","iopub.execute_input":"2022-01-08T16:38:15.242612Z","iopub.status.idle":"2022-01-08T16:38:15.2874Z","shell.execute_reply.started":"2022-01-08T16:38:15.242575Z","shell.execute_reply":"2022-01-08T16:38:15.286721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batch size of 8\nimages, annotations, scores = next(iter(train_loader))\nprint(images.shape)\nprint(scores.shape)\nprint(annotations.shape)","metadata":{"gradient":{"execution_count":446,"id":"2ffc7490","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:15.288775Z","iopub.execute_input":"2022-01-08T16:38:15.289485Z","iopub.status.idle":"2022-01-08T16:38:22.699263Z","shell.execute_reply.started":"2022-01-08T16:38:15.289446Z","shell.execute_reply":"2022-01-08T16:38:22.698342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Look at some images**","metadata":{}},{"cell_type":"code","source":"# Helper function to unnormalize and plot images\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image * np.array((0.229, 0.224, 0.225)).reshape(3, 1, 1) + np.array((0.485, 0.456, 0.406)).reshape(3, 1, 1)\n    img = (image * 255).astype(np.uint8) # unnormalize\n    \n\n    return plt.imshow(np.transpose(img, (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:22.701158Z","iopub.execute_input":"2022-01-08T16:38:22.701689Z","iopub.status.idle":"2022-01-08T16:38:22.708216Z","shell.execute_reply.started":"2022-01-08T16:38:22.701623Z","shell.execute_reply":"2022-01-08T16:38:22.707476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_numpy = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(8):\n    ax = fig.add_subplot(2, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(scores[idx].item())","metadata":{"gradient":{"execution_count":229,"id":"07100596","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:22.70978Z","iopub.execute_input":"2022-01-08T16:38:22.710352Z","iopub.status.idle":"2022-01-08T16:38:24.526421Z","shell.execute_reply.started":"2022-01-08T16:38:22.710233Z","shell.execute_reply":"2022-01-08T16:38:24.52557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up the model structure","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as func\nimport torch.optim as optim","metadata":{"gradient":{"execution_count":336,"id":"70f480f6","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:24.529565Z","iopub.execute_input":"2022-01-08T16:38:24.529914Z","iopub.status.idle":"2022-01-08T16:38:24.535261Z","shell.execute_reply.started":"2022-01-08T16:38:24.529869Z","shell.execute_reply":"2022-01-08T16:38:24.534595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Downloading the pretrained model**\n\nTo access the pretrained model in a kaggle notebook, download it via pytorch on a local notebook, save the model using torch.save.  Then upload it to your kaggle notebook as a dataset which you can then load via torch.load without having to connect to the internet.","metadata":{}},{"cell_type":"code","source":"# model = models.resnet50(pretrained=True)\n# torch.save(model, 'resnet50_pretrained.pt')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:24.536588Z","iopub.execute_input":"2022-01-08T16:38:24.537099Z","iopub.status.idle":"2022-01-08T16:38:24.544091Z","shell.execute_reply.started":"2022-01-08T16:38:24.537058Z","shell.execute_reply":"2022-01-08T16:38:24.543279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load the model, replace the output layer, and choose which layers to freeze/train**","metadata":{}},{"cell_type":"code","source":"# Load the pretrained resnet50\nmodel = torch.load(f'{model_dir}resnet50_pretrained.pt')\n\n# Disable gradients on all model parameters to freeze the weights\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace the final fully connected resnet layer with a 2 fc layer network and sigmoid output\nmodel.fc = nn.Sequential(nn.Linear(2048, 256),\n                         nn.ReLU(),\n                         nn.Linear(256, 1),\n                         nn.Sigmoid())\n\nfor param in model.fc.parameters():\n    param.requires_grad = True\n    \n# Unfreeze the last few layers of the model\n\nfor param in model.layer4.parameters():\n    param.requires_grad = True","metadata":{"gradient":{"execution_count":449,"id":"c4a436fe","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:26.326468Z","iopub.execute_input":"2022-01-08T16:38:26.326973Z","iopub.status.idle":"2022-01-08T16:38:29.170424Z","shell.execute_reply.started":"2022-01-08T16:38:26.326927Z","shell.execute_reply":"2022-01-08T16:38:29.1697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:31.789386Z","iopub.execute_input":"2022-01-08T16:38:31.789917Z","iopub.status.idle":"2022-01-08T16:38:31.796319Z","shell.execute_reply.started":"2022-01-08T16:38:31.789879Z","shell.execute_reply":"2022-01-08T16:38:31.795406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(13)\n\ncriterion = nn.MSELoss(reduction='sum')\n\n#Adam with L2 regularization\noptimizer = optim.AdamW(model.parameters(), lr=0.000025, weight_decay=50)\n\n# Learning rate decay\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [1, 2, 6], gamma=0.5)","metadata":{"gradient":{"execution_count":450,"id":"91b0e833","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:38:36.250266Z","iopub.execute_input":"2022-01-08T16:38:36.250933Z","iopub.status.idle":"2022-01-08T16:38:36.256826Z","shell.execute_reply.started":"2022-01-08T16:38:36.250896Z","shell.execute_reply":"2022-01-08T16:38:36.256135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out the forward pass on a single batch\n\nimages, annotations, scores = next(iter(train_loader))\nwith torch.no_grad():\n    train_loss = 0.0\n    output = model(images)*100 # convert sigmoid output to pawpularity scale\n    loss = criterion(output, scores)\n    math.sqrt(loss.item()/global_batch_size)\n\nprint(scores.dtype)\nprint(output.dtype)\nprint('Starting RMSE: ', torch.mean(output))\nprint('Prediction Standard Deviation: ', torch.std(output))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:38:42.366415Z","iopub.execute_input":"2022-01-08T16:38:42.366703Z","iopub.status.idle":"2022-01-08T16:39:01.442331Z","shell.execute_reply.started":"2022-01-08T16:38:42.366652Z","shell.execute_reply":"2022-01-08T16:39:01.441595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"# check if CUDA is available and set the training device\n\ntrain_on_gpu = torch.cuda.is_available()\ndevice = torch.cuda.get_device_name()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print(f'CUDA is available!  Training on GPU {device}...')","metadata":{"gradient":{"execution_count":453,"id":"edf14100","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"execution":{"iopub.status.busy":"2022-01-08T16:39:42.807191Z","iopub.execute_input":"2022-01-08T16:39:42.807464Z","iopub.status.idle":"2022-01-08T16:39:42.816281Z","shell.execute_reply.started":"2022-01-08T16:39:42.807432Z","shell.execute_reply":"2022-01-08T16:39:42.815565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model training loop**\n\nRun the training and validation steps for a fixed number of epochs, and save the model anytime the validation loss decreases.  ","metadata":{}},{"cell_type":"code","source":"# Training and validation loop\n\nif train_on_gpu:\n    model.cuda()\n\nn_epochs = 10\n\nvalid_loss_min = np.Inf # track change in validation loss\n\ntrain_losses, valid_losses = [], []\n\nfor epoch in range(1, n_epochs+1):\n    \n    start = time.time()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    # Stop training the convolutional layers after a certain point\n    if epoch > 3:\n        for param in model.layer4.parameters():\n            param.requires_grad = False\n    \n    ###################\n    # train the model #\n    ###################\n    # put in training mode (enable dropout)\n    model.train()\n    for images, annotations, scores in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(images)*100 # multiply by 100 the sigmoid output to 0-100 pawpularity scale\n        # print(output.dtype)\n        # print(scores.dtype)\n        # calculate the batch loss\n        loss = criterion(output, scores)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()\n        \n    ######################    \n    # validate the model #\n    ######################\n    # eval mode (no dropout)\n    model.eval()\n    with torch.no_grad():\n        for images, annotations, scores in valid_loader:\n            # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(images)*100 # multiply by 100 the sigmoid output to 0-100 pawpularity scale\n            # calculate the batch loss\n            loss = criterion(output, scores)\n            # update average validation loss \n            valid_loss += loss.item()\n    \n    # calculate RMSE\n    train_loss = math.sqrt(train_loss/len(train_loader.sampler))\n    valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n        \n    # increment learning rate decay\n    scheduler.step()\n    \n    # print training/validation statistics \n    # print(f'Epoch: {e}, {float(time.time() - start):.3f} seconds, lr={optimizer.lr}')\n    print('Epoch: {}, time: {:.1f}s, lr: {:.7f} \\tTraining Loss: {:.3f} \\tValidation Loss: {:.3f}'.format(\n        epoch, float(time.time() - start), current_lr, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.3f} --> {:.3f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), f'{working_dir}pawpularity_best_model.pt')\n        valid_loss_min = valid_loss    ","metadata":{"gradient":{"execution_count":478,"id":"597dbdfb","kernelId":"e054bc54-048d-46c0-933a-55d23cc13c60"},"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Diagnostics and performance","metadata":{}},{"cell_type":"code","source":"# Load the best performing model on the validation set\nmodel.load_state_dict(torch.load(f'{working_dir}pawpularity_best_model.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the distribution of predictions\n\npredictions = []\nscore_list = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, annotations, scores in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(images)*100\n        predictions.extend(list(output.cpu().detach().numpy().reshape(len(output),)))\n        score_list.extend(list(scores.cpu().detach().numpy().reshape(len(scores),)))\n        \n\npreds_df = pd.DataFrame({'preds': predictions})\npreds_df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Manually Check RMSE\n\ndiffs = np.array(score_list) - np.array(predictions)\nprint(math.sqrt((diffs @ diffs)/len(valid_loader.sampler)))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that manually increasing the variance doesn't help\n\nmean = np.mean(np.array(predictions))\nstddev = np.std((np.array(predictions)))\nprint(mean, stddev)\nupdated_normalized = 1.5*(predictions-mean)/stddev\nnew_predictions = updated_normalized+predictions\n\ndiffs = np.array(score_list) - np.array(new_predictions)\nprint(math.sqrt((diffs @ diffs)/len(valid_loader.sampler)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of validation predictions - if this is too narrow that's an issue\n\nn, bins, patches = plt.hist(predictions, 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Predicted Pawpularity Histogram')\nplt.xlim(0, 100)\nplt.ylim(0, .2)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The range could still be a greater, and the model is failing completely at predicting the highest ranked images that get a score of 100.  That said, it is producing a much higher range of predictions than the and feels like it would give useful information to users submitting photos.  ","metadata":{}},{"cell_type":"code","source":"# Histogram of validation set actual scores\n\nn, bins, patches = plt.hist(train_df.iloc[valid_idx, 13], 50, density=True, facecolor='g', alpha=0.75)\n\nplt.xlabel('Pawpularity')\nplt.ylabel('Frequency')\nplt.title('Actual Pawpularity Histogram')\nplt.xlim(0, 100)\nplt.ylim(0, .2)\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the losses\nfig = plt.figure()\nax = plt.axes()\nax.plot(list(range(1, len(train_losses))), train_losses[1:])\nax.plot(list(range(1, len(valid_losses))), valid_losses[1:]);\nprint(f'best score: {valid_loss_min}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show examples of images and predicted vs. actual scores","metadata":{}},{"cell_type":"code","source":"images, annotations, scores = next(iter(valid_loader))\nimages, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_plot = model(images).cpu()*100\nimages, annotations, scores = images.cpu(), annotations.cpu(), scores.cpu()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the images in the batch, along with the corresponding labels and predictions\n\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(12):\n    ax = fig.add_subplot(3, 4, idx+1, xticks=[], yticks=[])\n    im_convert(images[idx])\n    ax.set_title(f'Act: {round(scores[idx].item())} Pred: {round(output_plot[idx].item())}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use the model to predict the test dataset","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(f'{data_dir}test.csv')\ntest_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best performing model on the validation set\nmodel.load_state_dict(torch.load('pawpularity_best_model.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityTestDataset(Dataset):\n    \"\"\"Dataset connecting dog images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations]\n        return sample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load the test dataset (careful to use validation transforms without img augmentation)\n\ntest_data = PawpularityTestDataset(f'{data_dir}test.csv', f'{data_dir}test', transform=img_transforms_valid)\n\nbatch_size = min(len(test_data), global_batch_size)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=workers) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step through with a reasonable batch size and build up the output dataset\n\nmodel.eval()\noutputs = []\nfor images, annotations in test_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        images, annotations = images.cuda(), annotations.cuda()\n    test_output = model(images)*100\n    outputs.extend(list(test_output.cpu().detach().numpy().reshape(len(test_output),)))\n    \nimg_names = list( test_df.iloc[:, 0].values)\noutputs = [round(x, 2) for x in outputs]\n\noutput_df = pd.DataFrame({'Id': img_names, 'Pawpularity': outputs})\noutput_df.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write the output in the required format\noutput_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}